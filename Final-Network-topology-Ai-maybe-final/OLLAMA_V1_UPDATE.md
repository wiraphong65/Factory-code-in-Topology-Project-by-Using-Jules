# üîÑ ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡πÄ‡∏õ‡πá‡∏ô Ollama v1 API Format

## üìã ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á

### ‚úÖ **‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï AI Service ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Ollama v1 API**

**‡πÑ‡∏ü‡∏•‡πå:** `backend/app/ai_service.py`

#### 1. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô API Endpoint
```python
# ‡∏Å‡πà‡∏≠‡∏ô: /api/generate
# ‡∏´‡∏•‡∏±‡∏á: /v1/chat/completions
```

#### 2. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Request Format
```python
# ‡∏Å‡πà‡∏≠‡∏ô: ‡πÉ‡∏ä‡πâ "prompt" field
payload = {
    "model": self.model,
    "prompt": full_prompt,
    "stream": False,
    "options": {...}
}

# ‡∏´‡∏•‡∏±‡∏á: ‡πÉ‡∏ä‡πâ "messages" array (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÄ‡∏Å‡πà‡∏≤)
payload = {
    "model": self.model,
    "messages": [
        {
            "role": "user",
            "content": full_prompt
        }
    ],
    "stream": False,
    "options": {...}
}
```

#### 3. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Response Parsing
```python
# ‡∏Å‡πà‡∏≠‡∏ô: result.get("response", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ")
# ‡∏´‡∏•‡∏±‡∏á: result.get("choices", [{}])[0].get("message", {}).get("content", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ")
```

#### 4. ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Health Check
```python
# ‡∏Å‡πà‡∏≠‡∏ô: /api/tags
# ‡∏´‡∏•‡∏±‡∏á: /v1/models
```

### ‚úÖ **‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï Configuration**

**‡πÑ‡∏ü‡∏•‡πå:** `backend/app/config.py`
```python
# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô model ‡∏à‡∏≤‡∏Å llama2 ‡πÄ‡∏õ‡πá‡∏ô llama3.2
OLLAMA_MODEL: str = "llama3.2"
```

### ‚úÖ **‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï Documentation**

- ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï README.md
- ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï AI_SETUP_GUIDE.md
- ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï AI_SETUP_COMPLETE.md
- ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï backend/setup.py

## üß™ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‚úÖ **Ollama v1 Health Check: SUCCESS**
```
Status Code: 200
‚úÖ Ollama v1 API is working
```

### ‚úÖ **Ollama v1 Chat Completions: SUCCESS**
```
Status Code: 200
‚úÖ Ollama response: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡πá‡∏≠‡∏õ‡πÇ‡∏≠‡∏•‡∏≠‡∏à‡∏µ‡∏ô‡πÄ‡∏ô‡πá‡∏ï...
```

### ‚úÖ **Backend AI Integration: SUCCESS**
```
AI Health Status: 200
‚úÖ AI Health: {'status': 'healthy', 'ollama_connected': True, 'model': 'llama3.2', 'api_version': 'v1'}

AI Analysis Status: 200
‚úÖ AI Analysis: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å...
```

## üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÄ‡∏Å‡πà‡∏≤

### ‚úÖ **Format ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô**
```javascript
// ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÄ‡∏Å‡πà‡∏≤ (Express.js)
const ollamaRes = await axios.post("http://localhost:11434/v1/chat/completions", {
  model: "llama3.2",
  messages: [
    {
      role: "user",
      content: `‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå topology network ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢: ${JSON.stringify(userJson)}`
    }
  ]
});

// ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÉ‡∏´‡∏°‡πà (FastAPI + Python)
payload = {
    "model": "llama3.2",
    "messages": [
        {
            "role": "user",
            "content": full_prompt
        }
    ],
    "stream": False
}
```

### ‚úÖ **Model ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô**
- ‡πÉ‡∏ä‡πâ `llama3.2` ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ
- API endpoint ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô: `/v1/chat/completions`
- Response format ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

## üöÄ ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama
```bash
# Windows: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å https://ollama.ai/
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. ‡∏£‡∏±‡∏ô Ollama
```bash
ollama serve
```

### 3. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Model
```bash
ollama pull llama3.2
```

### 4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ollama ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
ollama run llama3.2 "Hello, how are you?"

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Backend AI
python test_ollama_v1.py
```

## üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï

### Backend:
- `backend/app/ai_service.py` - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô v1 API format
- `backend/app/config.py` - ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô model ‡πÄ‡∏õ‡πá‡∏ô llama3.2
- `backend/app/routers/ai.py` - ‡πÄ‡∏û‡∏¥‡πà‡∏° api_version info

### Documentation:
- `README.md` - ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï model ‡πÅ‡∏•‡∏∞ API version
- `AI_SETUP_GUIDE.md` - ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï model
- `AI_SETUP_COMPLETE.md` - ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï model
- `backend/setup.py` - ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï .env template

### Testing:
- `test_ollama_v1.py` - ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà

## üéâ ‡∏™‡∏£‡∏∏‡∏õ

**‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!** ‡∏£‡∏∞‡∏ö‡∏ö AI ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ Ollama v1 API format ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡πÄ‡∏Å‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:

‚úÖ **API Format:** `/v1/chat/completions`  
‚úÖ **Model:** `llama3.2`  
‚úÖ **Request Format:** `messages` array  
‚úÖ **Response Format:** `choices[0].message.content`  
‚úÖ **Health Check:** `/v1/models`  

**‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢!** üöÄ 